{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "crop() takes from 1 to 2 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mdata/weather-0/routes_town01_long_w0_06_23_09_24_57/topdown/0036.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mcrop(\u001b[39m128\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m384\u001b[39;49m, \u001b[39m256\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m image_torch \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(image)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m)\n\u001b[0;32m      8\u001b[0m image\n",
      "\u001b[1;31mTypeError\u001b[0m: crop() takes from 1 to 2 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "image = Image.open(\"data/weather-0/routes_town01_long_w0_06_23_09_24_57/topdown/0036.png\")\n",
    "image = image.convert(\"L\")\n",
    "image = image.crop((128, 0, 384, 256))\n",
    "image_torch = transforms.ToTensor()(image).reshape(1,1,256,256)\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adsfv\\.conda\\envs\\diff_example\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class Pad(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)\n",
    "\n",
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_in,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_out,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "        )\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in,\n",
    "                                       dim_out,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "    def forward(self, x):\n",
    "        #x -> [1, dim_in, resx, resy]\n",
    "        res = x\n",
    "        if self.res:\n",
    "            #[1, dim_in, resx, resy] -> [1, dim_in, resx, resy]\n",
    "            res = self.res(x)\n",
    "        #[1, dim_in, resx, resy] -> [1, dim_in, resx, resy]\n",
    "        return res + self.s(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.GroupNorm(num_channels=512,\n",
    "                                       num_groups=32,\n",
    "                                       eps=1e-6,\n",
    "                                       affine=True)\n",
    "        self.q = torch.nn.Linear(512, 512)\n",
    "        self.k = torch.nn.Linear(512, 512)\n",
    "        self.v = torch.nn.Linear(512, 512)\n",
    "        self.out = torch.nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 512, 32, 32]\n",
    "        res = x\n",
    "        #norm,维度不变\n",
    "        #[1, 512, 32, 32]\n",
    "        x = self.norm(x)\n",
    "        #[1, 512, 32, 32] -> [1, 512, 1024] -> [1, 1024, 512]\n",
    "        x = x.flatten(start_dim=2).transpose(1, 2)\n",
    "        #线性运算,维度不变\n",
    "        #[1, 1024, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        #[1, 1024, 512] -> [1, 512, 1024]\n",
    "        k = k.transpose(1, 2)\n",
    "        #[1, 1024, 512] * [1, 512, 1024] -> [1, 1024, 1024]\n",
    "        #0.044194173824159216 = 1 / 512**0.5\n",
    "        #atten = q.bmm(k) * 0.044194173824159216\n",
    "        #照理来说应该是等价的,但是却有很小的误差\n",
    "        atten = torch.baddbmm(torch.empty(1, 1024, 1024, device=q.device),\n",
    "                              q,\n",
    "                              k,\n",
    "                              beta=0,\n",
    "                              alpha=0.044194173824159216)\n",
    "        atten = torch.softmax(atten, dim=2)\n",
    "        #[1, 1024, 1024] * [1, 1024, 512] -> [1, 1024, 512]\n",
    "        atten = atten.bmm(v)\n",
    "        #线性运算,维度不变\n",
    "        #[1, 1024, 512]\n",
    "        atten = self.out(atten)\n",
    "        #[1, 1024, 512] -> [1, 512, 1024] -> [1, 512, 32, 32]\n",
    "        atten = atten.transpose(1, 2).reshape(-1, 512, 32, 32)\n",
    "        #残差连接,维度不变\n",
    "        #[1, 512, 32, 32]\n",
    "        atten = atten + res\n",
    "        return atten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 256, 256)\n",
    "encoder = torch.nn.Sequential(\n",
    "    # in\n",
    "    torch.nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "    # down\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(128, 128),\n",
    "        Resnet(128, 128),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(128, 256),\n",
    "        Resnet(256, 256),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(256, 256, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(256, 512),\n",
    "        Resnet(512, 512),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(512, 512, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(512, 512),\n",
    "        Resnet(512, 512),\n",
    "    ),\n",
    "    # mid\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(512, 512),\n",
    "        Atten(),\n",
    "        Resnet(512, 512),\n",
    "    ),\n",
    "        #out\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.GroupNorm(num_channels=512, num_groups=32, eps=1e-6),\n",
    "        torch.nn.SiLU(),\n",
    "        torch.nn.Conv2d(512, 8, 3, padding=1),\n",
    "    ),\n",
    "\n",
    "    #正态分布层\n",
    "    torch.nn.Conv2d(8, 8, 1),\n",
    ")\n",
    "x = encoder(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256]) torch.uint8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.randn(1, 1, 256, 256).byte()\n",
    "tensor_squeezed = tensor.squeeze()\n",
    "print(tensor_squeezed.shape,tensor_squeezed.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor1 = torch.ones(1, 256, 256)\n",
    "tensor2 = torch.ones(1, 256, 256)\n",
    "tensor3 = torch.ones(1, 256, 256)\n",
    "\n",
    "tensor = torch.cat((tensor1, tensor2, tensor3), dim=0)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adsfv\\Desktop\\毕设\\diff_study\\test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m../models\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m./dataset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvae\u001b[39;00m \u001b[39mimport\u001b[39;00m VAE\n\u001b[0;32m      9\u001b[0m vae_model \u001b[39m=\u001b[39m VAE(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m summary(vae_model, (\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import os,torch,sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('./models')\n",
    "sys.path.append('./dataset')\n",
    "from models.vae import VAE\n",
    "\n",
    "\n",
    "vae_model = VAE(1,1)\n",
    "summary(vae_model, (1, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adsfv\\.conda\\envs\\diff_example\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 15, 18,  ..., 12, 14,  1],\n",
      "        [ 3, 22,  9,  ..., 22,  0,  6],\n",
      "        [12, 17,  4,  ..., 17, 18,  3],\n",
      "        ...,\n",
      "        [10, 24, 11,  ..., 10,  0, 14],\n",
      "        [10,  9, 22,  ..., 18, 11, 25],\n",
      "        [19,  9, 21,  ..., 15, 25,  9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设张量为tensor\n",
    "tensor = torch.randn(26, 256, 256)  # 示例张量\n",
    "\n",
    "# 对第0个维度进行softmax操作\n",
    "softmax_tensor = torch.softmax(tensor, dim=0)\n",
    "\n",
    "# 获取第0个维度中最大值的索引\n",
    "max_index = torch.argmax(softmax_tensor, dim=0)\n",
    "\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "tensor(9, dtype=torch.int32)\n",
      "torch.Size([1, 256, 256, 26])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义原始矩阵\n",
    "tensor = torch.randint(0, 26, (256, 256), dtype=torch.uint8).unsqueeze(0).int()\n",
    "print(tensor.shape)\n",
    "print(tensor[0][1][1])\n",
    "ones = torch.eye(26,dtype=torch.int32)\n",
    "onehot = ones.index_select(0, tensor.view(-1)).reshape(1, 256, 256,26)\n",
    "print(onehot.shape)\n",
    "print(onehot[0][1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义原始矩阵\n",
    "tensor = torch.randint(0, 26, (256, 256), dtype=torch.uint8).unsqueeze(0).int()\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "ones = torch.eye(26)\n",
    "ones[3][3] = 100\n",
    "ones[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "90 91 92\n"
     ]
    }
   ],
   "source": [
    "x= b'\\x90\\x91\\x92'\n",
    "print(type(x))\n",
    "print(' '.join([hex(i)[2:].zfill(2) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adsfv\\.conda\\envs\\diff_example\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "a = torch.Tensor([[1, 2, np.nan], [2, np.nan, 4], [3, 4, 5]])\n",
    "torch.any(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'convert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bcd1bda0dfda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mimage_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mimage_front\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_front\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mimage_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mimage_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\diff_study\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\diff_study\\lib\\site-packages\\clip\\clip.py\u001b[0m in \u001b[0;36m_convert_image_to_rgb\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_convert_image_to_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'convert'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "image_full = Image.open(\"E:\\\\dataset\\\\weather-0\\\\data\\\\routes_town01_long_w0_06_23_02_24_19\\\\rgb_full\\\\0063.png\")\n",
    "image_front = image_full.crop((0, 0, 800, 600))\n",
    "# center crop front\n",
    "image_far = image_front.crop((200, 150, 600, 450))\n",
    "image_left = image_full.crop((0, 600, 800, 1200))\n",
    "image_right = image_full.crop((0, 1200, 800, 1800))\n",
    "\n",
    "image_front = preprocess(image_front).unsqueeze(0).to(device)\n",
    "image_left = preprocess(image_left).unsqueeze(0).to(device)\n",
    "image_right = preprocess(image_right).unsqueeze(0).to(device)\n",
    "image_far = preprocess(image_far).unsqueeze(0).to(device)\n",
    "\n",
    "image = torch.cat((image_front, image_left, image_right,image_far), dim=0)\n",
    "image.shape\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "image_features.shape  # [1, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from models.resnet import Resnet\n",
    "import torch\n",
    "\n",
    "class Pad(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(5, 64, kernel_size=3, stride=1, padding=1),\n",
    "    Resnet(64, 64),\n",
    "    Resnet(64, 64),\n",
    "    # torch.nn.Sequential(\n",
    "    #     Pad(),\n",
    "    #     torch.nn.Conv2d(64, 64, 3, stride=2, padding=0),\n",
    "    # ),\n",
    "    torch.nn.Conv2d(64, 77, kernel_size=3, stride=1, padding=1),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 5, 768)\n",
    "x = x.reshape(1, 5, 32, 24)\n",
    "y = model(x)\n",
    "y = y.reshape(1, 77, 32*24)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3,4],[5,6,7,8]]).reshape(-1)\n",
    "a.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value after * must be an iterable, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# image_encoder = ImageEncoder(77)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m unet \u001b[39m=\u001b[39m UNet()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m summary(unet,[(\u001b[39m4\u001b[39;49m,\u001b[39m32\u001b[39;49m,\u001b[39m32\u001b[39;49m),(\u001b[39m77\u001b[39;49m,\u001b[39m768\u001b[39;49m),(\u001b[39m1\u001b[39;49m)],batch_size\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[39m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[39m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39min_size)\u001b[39m.\u001b[39mtype(dtype) \u001b[39mfor\u001b[39;00m in_size \u001b[39min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[39m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[39m=\u001b[39m OrderedDict()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[39m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[39m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39min_size)\u001b[39m.\u001b[39mtype(dtype) \u001b[39mfor\u001b[39;00m in_size \u001b[39min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[39m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[39m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mTypeError\u001b[0m: Value after * must be an iterable, not int"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../dataset')\n",
    "from models.vae import VAE\n",
    "from models.unet import UNet\n",
    "from models.image_encoder import ImageEncoder\n",
    "\n",
    "# image_encoder = ImageEncoder(77)\n",
    "unet = UNet().to('cpu')\n",
    "summary(unet,[(4,32,32),(77,768),(1)],batch_size=-1,device=\"cpu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
