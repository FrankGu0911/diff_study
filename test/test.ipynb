{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "image = Image.open(\"data/weather-0/routes_town01_long_w0_06_23_09_24_57/topdown/0036.png\")\n",
    "image = image.convert(\"L\")\n",
    "image = image.crop((128, 128, 384, 384))\n",
    "image_torch = transforms.ToTensor()(image).reshape(1,1,256,256)\n",
    "image_torch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adsfv\\.conda\\envs\\diff_example\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class Pad(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)\n",
    "\n",
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_in,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_out,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "        )\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in,\n",
    "                                       dim_out,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "    def forward(self, x):\n",
    "        #x -> [1, dim_in, resx, resy]\n",
    "        res = x\n",
    "        if self.res:\n",
    "            #[1, dim_in, resx, resy] -> [1, dim_in, resx, resy]\n",
    "            res = self.res(x)\n",
    "        #[1, dim_in, resx, resy] -> [1, dim_in, resx, resy]\n",
    "        return res + self.s(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.GroupNorm(num_channels=512,\n",
    "                                       num_groups=32,\n",
    "                                       eps=1e-6,\n",
    "                                       affine=True)\n",
    "        self.q = torch.nn.Linear(512, 512)\n",
    "        self.k = torch.nn.Linear(512, 512)\n",
    "        self.v = torch.nn.Linear(512, 512)\n",
    "        self.out = torch.nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 512, 32, 32]\n",
    "        res = x\n",
    "        #norm,维度不变\n",
    "        #[1, 512, 32, 32]\n",
    "        x = self.norm(x)\n",
    "        #[1, 512, 32, 32] -> [1, 512, 1024] -> [1, 1024, 512]\n",
    "        x = x.flatten(start_dim=2).transpose(1, 2)\n",
    "        #线性运算,维度不变\n",
    "        #[1, 1024, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        #[1, 1024, 512] -> [1, 512, 1024]\n",
    "        k = k.transpose(1, 2)\n",
    "        #[1, 1024, 512] * [1, 512, 1024] -> [1, 1024, 1024]\n",
    "        #0.044194173824159216 = 1 / 512**0.5\n",
    "        #atten = q.bmm(k) * 0.044194173824159216\n",
    "        #照理来说应该是等价的,但是却有很小的误差\n",
    "        atten = torch.baddbmm(torch.empty(1, 1024, 1024, device=q.device),\n",
    "                              q,\n",
    "                              k,\n",
    "                              beta=0,\n",
    "                              alpha=0.044194173824159216)\n",
    "        atten = torch.softmax(atten, dim=2)\n",
    "        #[1, 1024, 1024] * [1, 1024, 512] -> [1, 1024, 512]\n",
    "        atten = atten.bmm(v)\n",
    "        #线性运算,维度不变\n",
    "        #[1, 1024, 512]\n",
    "        atten = self.out(atten)\n",
    "        #[1, 1024, 512] -> [1, 512, 1024] -> [1, 512, 32, 32]\n",
    "        atten = atten.transpose(1, 2).reshape(-1, 512, 32, 32)\n",
    "        #残差连接,维度不变\n",
    "        #[1, 512, 32, 32]\n",
    "        atten = atten + res\n",
    "        return atten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 256, 256)\n",
    "encoder = torch.nn.Sequential(\n",
    "    # in\n",
    "    torch.nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "    # down\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(128, 128),\n",
    "        Resnet(128, 128),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(128, 256),\n",
    "        Resnet(256, 256),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(256, 256, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(256, 512),\n",
    "        Resnet(512, 512),\n",
    "        torch.nn.Sequential(\n",
    "            Pad(),\n",
    "            torch.nn.Conv2d(512, 512, 3, stride=2, padding=0),\n",
    "        ),\n",
    "    ),\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(512, 512),\n",
    "        Resnet(512, 512),\n",
    "    ),\n",
    "    # mid\n",
    "    torch.nn.Sequential(\n",
    "        Resnet(512, 512),\n",
    "        Atten(),\n",
    "        Resnet(512, 512),\n",
    "    ),\n",
    "        #out\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.GroupNorm(num_channels=512, num_groups=32, eps=1e-6),\n",
    "        torch.nn.SiLU(),\n",
    "        torch.nn.Conv2d(512, 8, 3, padding=1),\n",
    "    ),\n",
    "\n",
    "    #正态分布层\n",
    "    torch.nn.Conv2d(8, 8, 1),\n",
    ")\n",
    "x = encoder(x)\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
